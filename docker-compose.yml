services:
  ollama:
    image: ollama/ollama:latest
    container_name: esg_ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_KEEP_ALIVE=24h
    volumes:
      - ollama:/root/.ollama
    profiles: ["cpu","gpu"]

  app:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: esg_rag_api
    restart: unless-stopped
    depends_on:
      - ollama
    environment:
      - RUN_ENV=docker
      - OLLAMA_BASE_URL=http://ollama:11434
      - LLM_PROVIDER=${LLM_PROVIDER}
      - OLLAMA_MODEL=${OLLAMA_MODEL}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL}
      - TOP_K=${TOP_K}
      - VECTOR_STORE=${VECTOR_STORE}
      - CHROMA_DIR=${CHROMA_DIR}
      - USE_RERANKER=${USE_RERANKER}
      - RERANKER_URL=${RERANKER_URL}
    volumes:
      - ./data:/app/data:ro
      - ./index:/app/index
      - hf_cache:/root/.cache/huggingface
      - ./src:/app/src:ro
    ports:
      - "8000:8000"
    profiles: ["cpu","gpu"]

  reranker:
    build:
      context: .
      dockerfile: Dockerfile
    command: python -m uvicorn src.reranker_service:app --host 0.0.0.0 --port 9000
    container_name: esg_reranker
    restart: unless-stopped
    environment:
      - RUN_ENV=docker
      - RERANKER_MODEL=${RERANKER_MODEL}
    volumes:
      - hf_cache:/root/.cache/huggingface
      - ./src:/app/src:ro
    ports:
      - "9000:9000"
    profiles: ["reranker","cpu","gpu"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

volumes:
  ollama:
  hf_cache:
